<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM - Adbean&#39;s Blog</title><meta name="Description" content="Adbean&#39;s Blog"><meta property="og:url" content="https://ad-bean.github.io/posts/paper-megatron-lm-v2/">
  <meta property="og:site_name" content="Adbean&#39;s Blog">
  <meta property="og:title" content="Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM">
  <meta property="og:description" content="Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM å§‘ä¸”ç®—ä½œ Megatron LM v2ï¼Œå› ä¸ºæ˜¯æ™šä¸€å¹´å‘è¡¨çš„ï¼Œç²—ç•¥è¿‡ä¸€ä¸‹ï¼Œå› ä¸ºéƒ½æ˜¯åŒæ ·çš„ motivation å’Œ background ç­‰ç­‰ çŸ¥ä¹ä¸€ç¯‡ä¸é”™çš„æ€»ç»“ ä½œè€… Jared çš„è§†é¢‘ä»‹ç» å…¶ä¸­å’Œ ZeRO çš„å¯¹">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-26T22:39:12-05:00">
    <meta property="article:modified_time" content="2025-02-26T22:39:12-05:00">
    <meta property="article:tag" content="Paper Reading">
    <meta property="article:tag" content="Deep Learning">
    <meta property="article:tag" content="Language Models">
    <meta property="article:tag" content="Training">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM">
  <meta name="twitter:description" content="Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM å§‘ä¸”ç®—ä½œ Megatron LM v2ï¼Œå› ä¸ºæ˜¯æ™šä¸€å¹´å‘è¡¨çš„ï¼Œç²—ç•¥è¿‡ä¸€ä¸‹ï¼Œå› ä¸ºéƒ½æ˜¯åŒæ ·çš„ motivation å’Œ background ç­‰ç­‰ çŸ¥ä¹ä¸€ç¯‡ä¸é”™çš„æ€»ç»“ ä½œè€… Jared çš„è§†é¢‘ä»‹ç» å…¶ä¸­å’Œ ZeRO çš„å¯¹">
<meta name="application-name" content="Adbean&#39;s Blog">
<meta name="apple-mobile-web-app-title" content="Adbean&#39;s Blog"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="../../Owl.ico"><link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png"><link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="../../site.webmanifest"><link rel="canonical" href="https://ad-bean.github.io/posts/paper-megatron-lm-v2/" /><link rel="prev" href="https://ad-bean.github.io/posts/paper-megatron-lm/" /><link rel="stylesheet" href="../../css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/ad-bean.github.io\/posts\/paper-megatron-lm-v2\/"
        },"genre": "posts","keywords": "Paper Reading, Deep Learning, Language Models, Training","wordcount":  1777 ,
        "url": "https:\/\/ad-bean.github.io\/posts\/paper-megatron-lm-v2\/","datePublished": "2025-02-26T22:39:12-05:00","dateModified": "2025-02-26T22:39:12-05:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Adbean"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="../../" title="Adbean&#39;s Blog"><img
        class="lazyload logo"
        src="../../svg/loading.min.svg"
        data-src="../../Owl.ico"
        data-srcset="../../Owl.ico, ../../Owl.ico 1.5x, ../../Owl.ico 2x"
        data-sizes="auto"
        alt="/Owl.ico"
        title="/Owl.ico" />Adbean&#39;s Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="../../"> ä¸»é¡µ </a><a class="menu-item" href="../../posts/"> æ–‡ç«  </a><a class="menu-item" href="../../tags/"> æ ‡ç­¾ </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="æœç´¢æ–‡ç« æ ‡é¢˜æˆ–å†…å®¹..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="æœç´¢">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="æ¸…ç©º">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="åˆ‡æ¢ä¸»é¢˜">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="../../" title="Adbean&#39;s Blog"><img
        class="lazyload logo"
        src="../../svg/loading.min.svg"
        data-src="../../Owl.ico"
        data-srcset="../../Owl.ico, ../../Owl.ico 1.5x, ../../Owl.ico 2x"
        data-sizes="auto"
        alt="/Owl.ico"
        title="/Owl.ico" />Adbean&#39;s Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="æœç´¢æ–‡ç« æ ‡é¢˜æˆ–å†…å®¹..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="æœç´¢">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="æ¸…ç©º">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        å–æ¶ˆ
                    </a>
                </div><a class="menu-item" href="../../" title="">ä¸»é¡µ</a><a class="menu-item" href="../../posts/" title="">æ–‡ç« </a><a class="menu-item" href="../../tags/" title="">æ ‡ç­¾</a><a href="javascript:void(0);" class="menu-item theme-switch" title="åˆ‡æ¢ä¸»é¢˜">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">ç›®å½•</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://github.com/ad-bean" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Adbean</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-02-26">2025-02-26</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;çº¦ 1777 å­—&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;é¢„è®¡é˜…è¯» 4 åˆ†é’Ÿ&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>ç›®å½•</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#efficient-large-scale-language-model-training-on-gpu-clusters-using-megatron-lm">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a></li>
    <li><a href="#abstract">ABSTRACT</a></li>
    <li><a href="#introduction">INTRODUCTION</a></li>
    <li><a href="#modes-of-parallelism">MODES OF PARALLELISM</a>
      <ul>
        <li><a href="#data-parallelism">Data Parallelism</a></li>
        <li><a href="#pipeline-model-parallelism">Pipeline Model Parallelism</a></li>
        <li><a href="#tensor-model-parallelism">Tensor Model Parallelism</a></li>
      </ul>
    </li>
    <li><a href="#performance-analysis-of-parallelizati-on-configurations">PERFORMANCE ANALYSIS OF PARALLELIZATI ON CONFIGURATIONS</a>
      <ul>
        <li><a href="#tensor-and-pipeline-model-parallelism">Tensor and Pipeline Model Parallelism</a></li>
        <li><a href="#data-and-model-parallelism">Data and Model Parallelism</a></li>
        <li><a href="#microbatch-size">Microbatch Size</a></li>
        <li><a href="#activation-recomputation">Activation Recomputation</a></li>
      </ul>
    </li>
    <li><a href="#implementation">IMPLEMENTATION</a></li>
    <li><a href="#evaluation">EVALUATION</a>
      <ul>
        <li><a href="#end-to-end-performance">End-to-End Performance</a></li>
        <li><a href="#comparison-to-zero-3">Comparison to ZeRO-3</a></li>
        <li><a href="#pipeline-parallelism">Pipeline Parallelism</a></li>
        <li><a href="#comparison-of-parallel-configurations">Comparison of Parallel Configurations</a></li>
        <li><a href="#microbatch-size-1">Microbatch Size</a></li>
        <li><a href="#activation-recomputation-1">Activation Recomputation</a></li>
        <li><a href="#scatter-gather-optimization">Scatter-Gather Optimization</a></li>
        <li><a href="#fused-operators">Fused Operators</a></li>
        <li><a href="#inter-node-communication-bandwidth">Inter-Node Communication Bandwidth</a></li>
        <li><a href="#checkpoint-loading-and-saving">Checkpoint Loading and Saving</a></li>
      </ul>
    </li>
    <li><a href="#related-work">RELATED WORK</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="efficient-large-scale-language-model-training-on-gpu-clusters-using-megatron-lm">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</h2>
<p>å§‘ä¸”ç®—ä½œ Megatron LM v2ï¼Œå› ä¸ºæ˜¯æ™šä¸€å¹´å‘è¡¨çš„ï¼Œç²—ç•¥è¿‡ä¸€ä¸‹ï¼Œå› ä¸ºéƒ½æ˜¯åŒæ ·çš„ motivation å’Œ background ç­‰ç­‰</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/19482552307" target="_blank" rel="noopener noreffer ">çŸ¥ä¹ä¸€ç¯‡ä¸é”™çš„æ€»ç»“</a></p>
<p>ä½œè€… Jared çš„<a href="https://youtu.be/gHaNUcS1_O4?si=EEgK2Y5_6fZtAJB8" target="_blank" rel="noopener noreffer ">è§†é¢‘ä»‹ç»</a></p>
<p>å…¶ä¸­å’Œ ZeRO çš„å¯¹æ¯”å¾ˆæœ‰æ„æ€ï¼Œååé‡é«˜äº†å¾ˆå¤šã€‚å‘¨æœ«çœ‹çœ‹ Gpipe å’Œ ZeRO</p>
</blockquote>
<h2 id="abstract">ABSTRACT</h2>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œç»“åˆèµ·æ¥ï¼Œä»¥æ‰©å±•åˆ°æ•°åƒä¸ª GPUã€‚</p>
<p>æå‡ºäº†ä¸€ç§æ–°é¢–çš„äº¤é”™æµæ°´çº¿è°ƒåº¦æ–¹æ³•ï¼Œå¯ä»¥åœ¨å†…å­˜å ç”¨ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œå°†ååé‡æé«˜ 10%ä»¥ä¸Šã€‚</p>
<h2 id="introduction">INTRODUCTION</h2>
<p>æ•°æ®å¹¶è¡Œæ‰©å±•é€šå¸¸æ•ˆæœè‰¯å¥½ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼š</p>
<ol>
<li>beyond a point, the per-GPU batch size becomes too small, reducing GPU utilization and increasing communication cost</li>
<li>the maximum number of devices that can be used is the batch size, limiting the number of accelerators that can be used for training.</li>
</ol>
<p>æ¨¡å‹å¹¶è¡ŒæŠ€æœ¯ã€å¼ é‡ï¼ˆå±‚å†…ï¼‰æ¨¡å‹å¹¶è¡ŒåŒ–</p>
<ol>
<li>å¼ é‡å¹¶è¡Œæ‰€éœ€çš„ all-reduce é€šä¿¡éœ€è¦é€šè¿‡æœåŠ¡å™¨é—´é“¾è·¯ï¼Œè¿™äº›é“¾è·¯æ¯”å¤š GPU æœåŠ¡å™¨å†…å¯ç”¨çš„é«˜å¸¦å®½ NVLink[9]æ…¢ï¼›</li>
<li>é«˜åº¦çš„æ¨¡å‹å¹¶è¡ŒåŒ–å¯èƒ½ä¼šäº§ç”Ÿå°çš„çŸ©é˜µä¹˜æ³•ï¼ˆGEMMsï¼‰ï¼Œå¯èƒ½ä¼šé™ä½ GPU åˆ©ç”¨ç‡ã€‚</li>
</ol>
<p>Pipeline model parallelism æµæ°´çº¿æ¨¡å‹å¹¶è¡ŒåŒ–ï¼Œä¸€ä¸ªæ‰¹æ¬¡è¢«æ‹†åˆ†ä¸ºè¾ƒå°çš„å¾®æ‰¹æ¬¡ï¼Œæ‰§è¡Œè¿‡ç¨‹åœ¨è¿™äº›å¾®æ‰¹æ¬¡ä¹‹é—´è¿›è¡Œæµæ°´çº¿å¤„ç†ã€‚</p>
<p>å°†æµæ°´çº¿å¹¶è¡Œã€å¼ é‡å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œç»“åˆèµ·æ¥ï¼Œè¿™ç§æŠ€æœ¯æˆ‘ä»¬ç§°ä¹‹ä¸º PTD-P</p>
<p>è¿˜ä¸ ZeRO[36]è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°ç”±äºè·¨èŠ‚ç‚¹é€šä¿¡è¾ƒå°‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ 1750 äº¿å’Œ 5300 äº¿å‚æ•°çš„æ¨¡å‹ä¸Šæ¯” ZeRO-3 é«˜å‡º 70%ã€‚è¿™äº›æ¨¡å‹å¤ªå¤§ï¼Œæ— æ³•å®¹çº³åœ¨å¤š GPU æœåŠ¡å™¨ä¸Šã€‚</p>
<h2 id="modes-of-parallelism">MODES OF PARALLELISM</h2>
<h3 id="data-parallelism">Data Parallelism</h3>
<p>æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹éƒ½æ‹¥æœ‰å®Œæ•´æ¨¡å‹çš„å‰¯æœ¬ï¼Œè¾“å…¥æ•°æ®é›†è¢«åˆ†ç‰‡ï¼Œå·¥ä½œèŠ‚ç‚¹å®šæœŸèšåˆå®ƒä»¬çš„æ¢¯åº¦ï¼Œä»¥ç¡®ä¿æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹çœ‹åˆ°ä¸€è‡´çš„æƒé‡ç‰ˆæœ¬</p>
<h3 id="pipeline-model-parallelism">Pipeline Model Parallelism</h3>
<p>åœ¨æµæ°´çº¿å¹¶è¡ŒåŒ–ä¸­ï¼Œ<strong>æ¨¡å‹çš„å„å±‚</strong>è¢«åˆ†é…åˆ°å¤šä¸ªè®¾å¤‡ä¸Šã€‚å½“ç”¨äºå…·æœ‰é‡å¤ç›¸åŒ Transformer å—çš„æ¨¡å‹æ—¶ï¼Œæ¯ä¸ªè®¾å¤‡å¯ä»¥è¢«åˆ†é…ç›¸åŒæ•°é‡çš„ Transformer å±‚ã€‚</p>
<p>ä¸€ä¸ªæ‰¹æ¬¡è¢«æ‹†åˆ†ä¸ºè¾ƒå°çš„å¾®æ‰¹æ¬¡ï¼›ç„¶ååœ¨å¾®æ‰¹æ¬¡ä¹‹é—´è¿›è¡Œæµæ°´çº¿æ‰§è¡Œã€‚</p>
<blockquote>
<p>å…¶å®æµæ°´çº¿å’Œ CPU çš„æµæ°´çº¿è°ƒåº¦å¾ˆç›¸ä¼¼ï¼Ÿ</p>
</blockquote>
<p>é‡åŒ– GPipe çš„æµæ°´çº¿æ°”æ³¡å¤§å°ï¼ˆğ‘¡ğ‘ğ‘ï¼‰ã€‚æˆ‘ä»¬å°†æ‰¹æ¬¡ä¸­çš„å¾®æ‰¹æ¬¡æ•°é‡è¡¨ç¤ºä¸º ğ‘šï¼Œæµæ°´çº¿é˜¶æ®µçš„æ•°é‡ï¼ˆç”¨äºæµæ°´çº¿å¹¶è¡Œçš„è®¾å¤‡æ•°é‡ï¼‰è¡¨ç¤ºä¸º ğ‘ï¼Œæ¯æ¬¡è¿­ä»£çš„ç†æƒ³æ—¶é—´è¡¨ç¤ºä¸º ğ‘¡ğ‘–ğ‘‘ï¼ˆå‡è®¾å®Œç¾æˆ–ç†æƒ³çš„æ‰©å±•ï¼‰ï¼Œæ‰§è¡Œå•ä¸ªå¾®æ‰¹æ¬¡çš„å‰å‘å’Œåå‘ä¼ é€’çš„æ—¶é—´è¡¨ç¤ºä¸º ğ‘¡ğ‘“ å’Œ ğ‘¡ğ‘ã€‚åœ¨æ­¤è°ƒåº¦ä¸­ï¼Œæµæ°´çº¿æ°”æ³¡åŒ…æ‹¬æ‰¹æ¬¡å¼€å§‹æ—¶çš„ ğ‘âˆ’1 ä¸ªå‰å‘ä¼ é€’å’Œæ‰¹æ¬¡ç»“æŸæ—¶çš„ ğ‘âˆ’1 ä¸ªåå‘ä¼ é€’ã€‚æµæ°´çº¿æ°”æ³¡ä¸­èŠ±è´¹çš„æ€»æ—¶é—´ä¸º ğ‘¡ğ‘ğ‘ = (ğ‘âˆ’1)Â·(ğ‘¡ğ‘“ +ğ‘¡ğ‘)ã€‚æ‰¹æ¬¡çš„ç†æƒ³å¤„ç†æ—¶é—´ä¸º ğ‘¡ğ‘–ğ‘‘ = ğ‘šÂ·(ğ‘¡ğ‘“ +ğ‘¡ğ‘)ã€‚å› æ­¤ï¼Œæµæ°´çº¿æ°”æ³¡ä¸­èŠ±è´¹çš„ç†æƒ³è®¡ç®—æ—¶é—´çš„æ¯”ä¾‹ä¸º</p>
<p>$$
Bubble\ time\ fraction (pipeline\ bubble\ size) = \frac{t_{pb}}{t_{id}} = \frac{(pâˆ’1)\cdot (t_f + t_b)}{m \cdot(t_f + t_b)} = \frac{pâˆ’1}{m}
$$</p>
<p>ä¸ºäº†å‡å°‘æµæ°´çº¿æ°”æ³¡çš„å½±å“ï¼Œå¯ä»¥<strong>å¢åŠ å¾®æ‰¹æ¬¡çš„æ•°é‡</strong>æˆ–<strong>å‡å°‘æµæ°´çº¿é˜¶æ®µçš„æ•°é‡</strong>ã€‚</p>
<p>ä½¿æµæ°´çº¿æ°”æ³¡æ—¶é—´å æ¯”ï¼ˆbubble time fractionï¼‰å°½å¯èƒ½å°ï¼Œæˆ‘ä»¬éœ€è¦æ»¡è¶³ $m &raquo; p$</p>
<p>Schedule with Interleaved Stages: æ–°è°ƒåº¦å°†æ°”æ³¡æ—¶é—´å‡å°‘äº† v å€ï¼Œè¿™ç§æµæ°´çº¿æ°”æ³¡å¤§å°çš„å‡å°‘å¹¶éæ²¡æœ‰ä»£ä»·ï¼šè¿™ç§è°ƒåº¦éœ€è¦é¢å¤–çš„é€šä¿¡ã€‚</p>
<h3 id="tensor-model-parallelism">Tensor Model Parallelism</h3>
<p><img
        class="lazyload"
        src="../../svg/loading.min.svg"
        data-src="https://s2.loli.net/2025/02/27/XAPO3Kie1fxMUlS.png"
        data-srcset="https://s2.loli.net/2025/02/27/XAPO3Kie1fxMUlS.png, https://s2.loli.net/2025/02/27/XAPO3Kie1fxMUlS.png 1.5x, https://s2.loli.net/2025/02/27/XAPO3Kie1fxMUlS.png 2x"
        data-sizes="auto"
        alt="https://s2.loli.net/2025/02/27/XAPO3Kie1fxMUlS.png"
        title="https://s2.loli.net/2025/02/27/XAPO3Kie1fxMUlS.png" /></p>
<ol>
<li>MLP parallelism</li>
<li>multi-head attention parallelism</li>
</ol>
<h2 id="performance-analysis-of-parallelizati-on-configurations">PERFORMANCE ANALYSIS OF PARALLELIZATI ON CONFIGURATIONS</h2>
<h3 id="tensor-and-pipeline-model-parallelism">Tensor and Pipeline Model Parallelism</h3>
<p>$$</p>
<p>\frac{p - 1}{m} = \frac{n / t - 1}{m}
$$</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ<strong>å¼ é‡æ¨¡å‹å¹¶è¡ŒåŒ–</strong>å¢åŠ äº†è®¾å¤‡ä¹‹é—´çš„é€šä¿¡é‡ã€‚å› æ­¤ï¼Œå½“ ğ‘¡ å¤§äºå•ä¸ªèŠ‚ç‚¹ä¸­çš„ GPU æ•°é‡æ—¶ï¼Œè·¨è¾ƒæ…¢çš„èŠ‚ç‚¹é—´é“¾è·¯æ‰§è¡Œå¼ é‡æ¨¡å‹å¹¶è¡ŒåŒ–çš„å¼€é”€å¯èƒ½æ˜¯ä¸åˆ‡å®é™…çš„ã€‚</p>
<blockquote>
<p>ğ‘¡ è¡¨ç¤ºå¼ é‡æ¨¡å‹å¹¶è¡Œçš„è§„æ¨¡ï¼Œğ‘ è¡¨ç¤ºæµæ°´çº¿æ¨¡å‹å¹¶è¡Œçš„è§„æ¨¡ï¼Œğ‘‘ è¡¨ç¤ºæ•°æ®å¹¶è¡Œçš„è§„æ¨¡ã€‚</p>
<p>ğ‘ï¼šå¾®æ‰¹æ¬¡å¤§å°
ğ‘šï¼šæ¯ä¸ªæµæ°´çº¿ä¸­ä¸€ä¸ªæ‰¹æ¬¡å†…çš„å¾®æ‰¹æ¬¡æ•°é‡
ğ‘›ï¼šGPU çš„æ•°é‡</p>
</blockquote>
<h3 id="data-and-model-parallelism">Data and Model Parallelism</h3>
<p>åœ¨ä½¿ç”¨å¼ é‡æ¨¡å‹å¹¶è¡ŒåŒ–æ—¶ï¼Œæ¯ä¸ªå¾®æ‰¹æ¬¡éƒ½éœ€è¦æ‰§è¡Œ all-reduce é€šä¿¡ã€‚è¿™åœ¨è·¨å¤š GPU æœåŠ¡å™¨æ—¶å¯èƒ½ä¼šéå¸¸æ˜‚è´µ</p>
<h3 id="microbatch-size">Microbatch Size</h3>
<p>å¾®æ‰¹æ¬¡å¤§å° b çš„é€‰æ‹©ä¹Ÿä¼šå½±å“æ¨¡å‹è®­ç»ƒçš„ååé‡ã€‚</p>
<h3 id="activation-recomputation">Activation Recomputation</h3>
<p>æ¿€æ´»é‡è®¡ç®—ï¼ˆActivation Recomputationï¼‰[12, 18, 20, 21] æ˜¯ä¸€ç§å¯é€‰æŠ€æœ¯ï¼Œé€šè¿‡åœ¨åå‘ä¼ é€’ä¹‹å‰å†æ¬¡è¿è¡Œå‰å‘ä¼ é€’ï¼ˆå¹¶ä»…ä¿å­˜ç»™å®šæµæ°´çº¿é˜¶æ®µçš„è¾“å…¥æ¿€æ´»å€¼ï¼Œè€Œä¸æ˜¯æ•´ä¸ªä¸­é—´æ¿€æ´»å€¼é›†ï¼Œåè€…å ç”¨å†…å­˜æ›´å¤§ï¼‰ï¼Œä»¥å¢åŠ è®¡ç®—æ“ä½œçš„æ•°é‡ä¸ºä»£ä»·æ¥å‡å°‘å†…å­˜å ç”¨ã€‚</p>
<h2 id="implementation">IMPLEMENTATION</h2>
<blockquote>
<p>ç•¥</p>
</blockquote>
<h2 id="evaluation">EVALUATION</h2>
<ul>
<li>
<p>PTD-P çš„æ€§èƒ½å¦‚ä½•ï¼Ÿå®ƒæ˜¯å¦èƒ½å®ç°å®é™…çš„ç«¯åˆ°ç«¯è®­ç»ƒæ—¶é—´ï¼Ÿ</p>
</li>
<li>
<p>æµæ°´çº¿å¹¶è¡ŒåŒ–åœ¨ç»™å®šæ¨¡å‹å’Œæ‰¹é‡å¤§å°ä¸‹çš„æ‰©å±•æ€§å¦‚ä½•ï¼Ÿäº¤é”™è°ƒåº¦å¯¹æ€§èƒ½æœ‰å¤šå¤§å½±å“ï¼Ÿ</p>
</li>
<li>
<p>ä¸åŒçš„å¹¶è¡ŒåŒ–ç»´åº¦ä¹‹é—´å¦‚ä½•äº¤äº’ï¼Ÿå¾®æ‰¹æ¬¡å¤§å°ç­‰è¶…å‚æ•°çš„å½±å“æ˜¯ä»€ä¹ˆï¼Ÿ</p>
</li>
<li>
<p>scatter-gather é€šä¿¡ä¼˜åŒ–çš„å½±å“æ˜¯ä»€ä¹ˆï¼Ÿåœ¨å¤§è§„æ¨¡è¿è¡Œè®­ç»ƒè¿­ä»£æ—¶ï¼Œæˆ‘ä»¬å¯¹ç¡¬ä»¶æ–½åŠ äº†å“ªäº›é™åˆ¶ï¼Ÿ</p>
</li>
</ul>
<h3 id="end-to-end-performance">End-to-End Performance</h3>
<p>è¯„ä¼°äº†ç³»ç»Ÿåœ¨å‚æ•°é‡ä» 10 äº¿åˆ° 1 ä¸‡äº¿çš„ GPT æ¨¡å‹ä¸Šçš„ç«¯åˆ°ç«¯æ€§èƒ½</p>
<p>å¯ç”¨äº† scatter/gather ä¼˜åŒ– çš„äº¤é”™æµæ°´çº¿è°ƒåº¦</p>
<blockquote>
<p>å¦‚ä½•è®¡ç®— FLOPSï¼Ÿ</p>
</blockquote>
<h3 id="comparison-to-zero-3">Comparison to ZeRO-3</h3>
<p>PTD-P åœ¨è¿™ä¸¤ä¸ªæ¨¡å‹ä¸Šçš„æ€§èƒ½æ¯” ZeRO-3 é«˜å‡º 70%ï¼Œè¿™å¾—ç›Šäºæ›´å°‘çš„è·¨èŠ‚ç‚¹é€šä¿¡ã€‚</p>
<h3 id="pipeline-parallelism">Pipeline Parallelism</h3>
<p>Weak Scaling. è¾ƒé«˜çš„æ‰¹é‡å¤§å°å…·æœ‰æ›´å¥½çš„æ‰©å±•æ€§ï¼Œå› ä¸ºæµæ°´çº¿æ°”æ³¡è¢«åˆ†æ‘Šåˆ°æ›´å¤šçš„å¾®æ‰¹æ¬¡ä¸Š</p>
<p>Interleaved versus Non-Interleaved Schedule. å¯ç”¨ scatter/gather é€šä¿¡ä¼˜åŒ– çš„äº¤é”™è°ƒåº¦æ¯”éäº¤é”™ï¼ˆé»˜è®¤ï¼‰è°ƒåº¦å…·æœ‰æ›´é«˜çš„è®¡ç®—æ€§èƒ½ã€‚</p>
<h3 id="comparison-of-parallel-configurations">Comparison of Parallel Configurations</h3>
<h3 id="microbatch-size-1">Microbatch Size</h3>
<h3 id="activation-recomputation-1">Activation Recomputation</h3>
<h3 id="scatter-gather-optimization">Scatter-Gather Optimization</h3>
<h3 id="fused-operators">Fused Operators</h3>
<h3 id="inter-node-communication-bandwidth">Inter-Node Communication Bandwidth</h3>
<h3 id="checkpoint-loading-and-saving">Checkpoint Loading and Saving</h3>
<blockquote>
<p>å„ç§é…ç½® evaluation ç•¥</p>
</blockquote>
<h2 id="related-work">RELATED WORK</h2>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>æ›´æ–°äº 2025-02-26</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="åˆ†äº«åˆ° Twitter" data-sharer="twitter" data-url="https://ad-bean.github.io/posts/paper-megatron-lm-v2/" data-title="Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM" data-hashtags="Paper Reading,Deep Learning,Language Models,Training"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° Facebook" data-sharer="facebook" data-url="https://ad-bean.github.io/posts/paper-megatron-lm-v2/" data-hashtag="Paper Reading"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° Hacker News" data-sharer="hackernews" data-url="https://ad-bean.github.io/posts/paper-megatron-lm-v2/" data-title="Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° Line" data-sharer="line" data-url="https://ad-bean.github.io/posts/paper-megatron-lm-v2/" data-title="Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="åˆ†äº«åˆ° å¾®åš" data-sharer="weibo" data-url="https://ad-bean.github.io/posts/paper-megatron-lm-v2/" data-title="Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="../../tags/paper-reading/">Paper Reading</a>,&nbsp;<a href="../../tags/deep-learning/">Deep Learning</a>,&nbsp;<a href="../../tags/language-models/">Language Models</a>,&nbsp;<a href="../../tags/training/">Training</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">è¿”å›</a></span>&nbsp;|&nbsp;<span><a href="../../">ä¸»é¡µ</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="../../posts/paper-megatron-lm/" class="prev" rel="prev" title="Paper Reading: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Paper Reading: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">ç”± <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.129.0">Hugo</a> å¼ºåŠ›é©±åŠ¨ | ä¸»é¢˜ - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2022 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://github.com/ad-bean" target="_blank">Adbean</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="å›åˆ°é¡¶éƒ¨">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="æŸ¥çœ‹è¯„è®º">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"å¤åˆ¶åˆ°å‰ªè´´æ¿","maxShownLines":50},"comment":{},"search":{"algoliaAppID":"OKDXG1V1ML","algoliaIndex":"blog","algoliaSearchKey":"d36cbd4755f8bf0c3fc49bb46f4b8f8a","highlightTag":"em","maxResultLength":10,"noResultsFound":"æ²¡æœ‰æ‰¾åˆ°ç»“æœ","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="../../js/theme.min.js"></script></body>
</html>
