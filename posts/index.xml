<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - Adbean&#39;s Blog</title>
        <link>https://ad-bean.github.io/posts/</link>
        <description>所有文章 | Adbean&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>adbeanx@outlook.com (Adbean)</managingEditor>
            <webMaster>adbeanx@outlook.com (Adbean)</webMaster><lastBuildDate>Sat, 08 Mar 2025 15:13:36 -0500</lastBuildDate><atom:link href="https://ad-bean.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>CMU 10-414/714: Deep Learning Systems (2020) - 深度学习系统 hw0</title>
    <link>https://ad-bean.github.io/posts/ml-sys-hw0/</link>
    <pubDate>Tue, 04 Mar 2025 16:46:50 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/ml-sys-hw0/</guid>
    <description><![CDATA[10-714: Homework 0 build a basic softmax regression algorithm, plus a simple two-layer neural network github hw0 Question 1: A basic add function, and testing/autograding basics plement simple_ml.add() function in src/simple_ml.py testing: !python3 -m pytest -k &quot;add&quot; 根据测试，由于 add(x, y) 传入的参数可以是任意类型，则直接返回 return x + y 没有]]></description>
</item>
<item>
    <title>CMU 10-414/714: Deep Learning Systems (2020) - 深度学习系统 04 Automatic Differentiation</title>
    <link>https://ad-bean.github.io/posts/ml-sys-03/</link>
    <pubDate>Mon, 03 Mar 2025 00:49:41 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/ml-sys-03/</guid>
    <description><![CDATA[Automatic Differentiation hypothesis class: $x \rightarrow h_\theta(x)$, MLP loss function(cross-entropy loss): $\ell(x, y) = -h_y(x) + \log \sum_{j=1}^n \exp(h_j(x))$ optimization method: $\theta := \theta - \alpha \nabla_\theta \ell$ 机器学习/深度学习是否就是在学习参数集合 $\theta$? 除了 SGD 随机梯度下降，还有 Adam 等优化方法 计算 gradient]]></description>
</item>
<item>
    <title>Paper Reading: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</title>
    <link>https://ad-bean.github.io/posts/paper-megatron-lm-v2/</link>
    <pubDate>Wed, 26 Feb 2025 22:39:12 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/paper-megatron-lm-v2/</guid>
    <description><![CDATA[Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM 姑且算作 Megatron LM v2，因为是晚一年发表的，粗略过一下，因为都是同样的 motivation 和 background 等等 知乎一篇不错的总结 作者 Jared 的视频介绍 其中和 ZeRO 的对]]></description>
</item>
<item>
    <title>Paper Reading: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
    <link>https://ad-bean.github.io/posts/paper-megatron-lm/</link>
    <pubDate>Wed, 26 Feb 2025 13:19:30 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/paper-megatron-lm/</guid>
    <description><![CDATA[Megatron-LM Nvidia 开源的 Megatron-LM 大模型训练框架 结合 Model Parallelism 和 Pipeline Parallelism 实现了 Tensor Model Parallelism 基于 Transformer 和 Attention 进行切分，同样是经典的一篇分布式语言模型训练的文章 论文比较短，细节很少，需要结]]></description>
</item>
<item>
    <title>Paper Reading: PipeDream: Generalized Pipeline Parallelism for DNN Training [SOSP2019]</title>
    <link>https://ad-bean.github.io/posts/paper-pipedream/</link>
    <pubDate>Mon, 24 Feb 2025 10:49:51 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/paper-pipedream/</guid>
    <description><![CDATA[PipeDream: Generalized Pipeline Parallelism for DNN Training 第一次看 ML/DL (distributed) training 框架相关的论文，有很多地方不理解。 对 Evaluation 与数学证明更是浅尝辄止，许多指标和算法难以理解，关于大模型、分布式训练等等]]></description>
</item>
<item>
    <title>CMU 10-414/714: Deep Learning Systems (2020) - 深度学习系统 02-03 Neural Networks</title>
    <link>https://ad-bean.github.io/posts/ml-sys-02/</link>
    <pubDate>Mon, 10 Feb 2025 14:58:18 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/ml-sys-02/</guid>
    <description><![CDATA[&ldquo;Manual&rdquo; Neural Networks / Backprop 还是复习 ML 的内容 From linear to nonlinear hypothesis classes Neural networks Backpropagation (i.e., computiing gradients) hypothesis classes 是用于模型训练的函数类型 线性假设类、非线性假设类 多项式回归或神经网络等模型都属于非线性]]></description>
</item>
<item>
    <title>CMU 10-414/714: Deep Learning Systems (2020) - 深度学习系统 01 Softmax</title>
    <link>https://ad-bean.github.io/posts/ml-sys-01/</link>
    <pubDate>Wed, 05 Feb 2025 11:04:50 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/ml-sys-01/</guid>
    <description><![CDATA[Deep Learning Systems https://dlsyscourse.org/ 10-414/714: Deep Learning Systems 这学期选了 ML systems，一些 DL 的概念不太熟悉，补一下 Tianqi Chen 大神的课 Introduction AlexNet: Image Classification AlphaGo, StyleGAN, GPT-3, stable diffusion &hellip; DL 是深度神经网络，越深越好吗？ transformer 和 GAN]]></description>
</item>
<item>
    <title>阅读笔记：微服务架构</title>
    <link>https://ad-bean.github.io/posts/backend-microservice/</link>
    <pubDate>Wed, 25 Dec 2024 15:27:29 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/backend-microservice/</guid>
    <description><![CDATA[微服务架构 回顾下微服务架构的一些知识，包括服务发现、负载均衡、可用性，很多东西没有实操过所以难以记住，需要多多回顾。 来自极客时间的课程 https://time.geekbang.org/column/intro/100551601 后端]]></description>
</item>
<item>
    <title>Paper Reading: From Cloud Computing to Sky Computing</title>
    <link>https://ad-bean.github.io/posts/paper-skycomputing/</link>
    <pubDate>Wed, 04 Dec 2024 11:25:04 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/paper-skycomputing/</guid>
    <description><![CDATA[From Cloud Computing to Sky Computing 21 年 UCB 提出了 Sky Computing 的概念，后续也有一篇 35 页的 The Sky Above The Clouds 相比起传统的云服务，sky computing 更像一个大一统的中间层，比如可以调用不同的云服务]]></description>
</item>
<item>
    <title>Paper Reading: ServiceRouter: Hyperscale and Minimal Cost Service Mesh at Meta</title>
    <link>https://ad-bean.github.io/posts/paper-servicerouter/</link>
    <pubDate>Mon, 02 Dec 2024 09:37:05 -0500</pubDate>
    <author>Adbean</author>
    <guid>https://ad-bean.github.io/posts/paper-servicerouter/</guid>
    <description><![CDATA[ServiceRouter: Hyperscale and Minimal Cost Service Mesh at Meta Meta 的全球服务网格 ServiceRouter（SR） 服务网格（Service Mesh）是一种用于处理微服务架构中服务之间通信]]></description>
</item>
</channel>
</rss>
